{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bb1ec9",
   "metadata": {},
   "source": [
    "Note: this currently runs terribly slow. I'm attempting to speed up this with numpy but it's a WIP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b037db",
   "metadata": {},
   "source": [
    "Data preparation: The 28x28 pixel images are flattened into a 784-dimensional vector. The pixel values are often normalized by dividing them by 255.\n",
    "Model architecture: An MLP for MNIST typically includes:\n",
    "\n",
    "    An input layer with 784 nodes, one for each pixel.\n",
    "    One or more hidden layers with a specified number of neurons, often using an activation function like ReLU.\n",
    "    An output layer with 10 nodes, representing the 10 possible digits (0-9), usually with a Softmax activation function to produce probabilities.\n",
    "\n",
    "Training: The model is trained using a large dataset of labeled images. The process involves:\n",
    "\n",
    "    Forward propagation: The input image vector is passed through the network to get an output prediction.\n",
    "    Backpropagation: The difference between the prediction and the actual label is calculated, and this error is used to adjust the model's weights to improve future predictions.\n",
    "    This process is repeated for multiple epochs, which are full passes through the entire dataset.\n",
    "\n",
    "Evaluation: After training, the model's performance is evaluated on a separate, unseen test set to determine its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed838f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "\n",
    "import pea\n",
    "from pea import nn\n",
    "import random\n",
    "\n",
    "# inspecting dataset before committing to download it\n",
    "# ds_builder = ds.load_dataset_builder(\"ylecun/mnist\")\n",
    "# ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ddc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset splits\n",
    "train_ds = ds.load_dataset(\"ylecun/mnist\", split=\"train\")\n",
    "test_ds = ds.load_dataset(\"ylecun/mnist\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting first image in test set\n",
    "print(test_ds[0][\"label\"])\n",
    "test_ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f867f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: convert png image to rgb\n",
    "train_ds = train_ds.cast_column(\"image\", ds.features.Image(mode=\"RGB\"))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: convert rgb image to grayscale pixel values and then value\n",
    "def rgb_to_value(examples):\n",
    "    out = []\n",
    "    for image in examples[\"image\"]:\n",
    "        # convert to rgb optional but best practice\n",
    "        rgb = list(image.convert(\"RGB\").getdata())\n",
    "        # 0-255 to 0.0-1.0\n",
    "        rgb_norm = [(r / 255.0, g / 255.0, b / 255.0) for r, g, b in rgb]\n",
    "        # convert to grayscale with luma formula: perceived brightness 0.0-1.0\n",
    "        gray = [0.2989 * r + 0.5870 * g + 0.1140 * b for r, g, b in rgb_norm]\n",
    "        value = [pea.Value(g) for g in gray]\n",
    "        out.append(value)\n",
    "    examples[\"value\"] = out\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_ds = train_ds.with_transform(rgb_to_value)\n",
    "train_ds[0][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99be0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.MLP(28 * 28, [128, 64, 10])\n",
    "\n",
    "    def __call__(self, x: Iterable[pea.Value]) -> list[pea.Value]:\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def parameters(self) -> list[pea.Value]:\n",
    "        return self.mlp.parameters()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"MNIST of 1 MLP [\\n{self.mlp.__repr__()}\\n]\"\n",
    "\n",
    "\n",
    "model = MNIST()\n",
    "print(str(model))\n",
    "print(len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataset: ds.Dataset,\n",
    "    *,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    lr: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Epoch means one full pass over the dataset. Batch size means how many examples\n",
    "    to process before updating the model parameters. Loss is averaged over the entire\n",
    "    dataset for reporting.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch = dataset[i : i + batch_size]\n",
    "            batch_loss = pea.Value(0.0)\n",
    "            for j in range(batch_size):\n",
    "                x = batch[\"value\"][j]\n",
    "                y = batch[\"label\"][j]\n",
    "                pred = model(x)\n",
    "                one_hot_y = nn.one_hot(pea.Value(y), num_classes=10)\n",
    "                sample_loss = nn.cross_entropy_loss(pred, one_hot_y)\n",
    "                batch_loss += sample_loss\n",
    "                print(f\"        Sample {j + 1}/{batch_size}, Loss: {sample_loss.data}\")\n",
    "            avg_batch_loss = batch_loss / batch_size\n",
    "            total_loss += avg_batch_loss.data\n",
    "            # backward\n",
    "            model.zero_grad()\n",
    "            avg_batch_loss.backward()\n",
    "            # update\n",
    "            for p in model.parameters():\n",
    "                p.data -= lr * p.grad\n",
    "            print(\n",
    "                f\"    Batch {i // batch_size + 1}/{(len(dataset) + batch_size - 1) // batch_size}, Batch Avg Loss: {avg_batch_loss.data}\"\n",
    "            )\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Epoch Avg Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "train(model, train_ds, epochs=1, batch_size=32, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e36d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test dataset\n",
    "test_ds = test_ds.cast_column(\"image\", ds.features.Image(mode=\"RGB\"))\n",
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.with_transform(rgb_to_value)\n",
    "test_ds[0][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25853273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, dataset: ds.Dataset) -> float:\n",
    "    correct = 0\n",
    "    for i in range(len(dataset)):\n",
    "        x = dataset[i][\"value\"]\n",
    "        y = dataset[i][\"label\"]\n",
    "        pred = model(x)\n",
    "        pred_label = max(range(len(pred)), key=lambda k: pred[k].data)\n",
    "        if pred_label == y:\n",
    "            correct += 1\n",
    "        print(f\"Predicted: {pred_label}, Actual: {y}, Accuracy so far: {(correct / (i + 1) * 100):.3g}%\")\n",
    "    accuracy = correct / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "accuracy = evaluate(model, test_ds)\n",
    "print(f\"Test set accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
