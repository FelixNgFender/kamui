{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f41b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "from typing import Literal\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a3b91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_MPS = False\n",
    "TORCH_SEED = 2147483647\n",
    "SEED = 42\n",
    "NAMES_FILE = \"names.txt\"\n",
    "TERM_TOK = \".\"\n",
    "MODEL_INITIAL_PATH = \"model_initial.pth\"\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "assert TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT == 1.0\n",
    "\n",
    "# hyperparams\n",
    "USE_BATCHNORM = True\n",
    "CONTEXT_SIZE = 3\n",
    "EMBEDDING_SIZE = 10\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_LAYERS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-1\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "device = (\n",
    "    torch.accelerator.current_accelerator() if torch.accelerator.is_available() and USE_MPS else torch.device(\"cpu\")\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d70a210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [B, C]: torch.Size([64, 3])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Vocab size: 27\n",
      "train dataset size: 182625\n",
      "dev dataset size: 22655\n",
      "test dataset size: 22866\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "class TrigramDataset(data.Dataset):\n",
    "    def __init__(self, words: list[str], context_size: int, stoi: dict[str, int], terminal_token: str):\n",
    "        self.context_size = context_size\n",
    "        self.stoi = stoi\n",
    "        self.data: list[tuple[list[str], int]] = []  # (context, label) tuples\n",
    "\n",
    "        for word in words:\n",
    "            context: list[int] = [0] * context_size\n",
    "            for ch in word + terminal_token:\n",
    "                ix = stoi[ch]\n",
    "                self.data.append((context.copy(), ix))\n",
    "                context = context[1:] + [ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        context, label = self.data[idx]\n",
    "        return torch.tensor(context), torch.tensor(label)\n",
    "\n",
    "\n",
    "words = [line.strip() for line in pathlib.Path(NAMES_FILE).open(\"r\").readlines()]\n",
    "random.shuffle(words)\n",
    "chars = [TERM_TOK] + sorted(list(set(\"\".join(words))))\n",
    "vocab_size = len(chars)\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = chars\n",
    "\n",
    "\n",
    "n1 = int(TRAIN_SPLIT * len(words))\n",
    "n2 = int((TRAIN_SPLIT + VAL_SPLIT) * len(words))\n",
    "\n",
    "train_dataset = TrigramDataset(words[:n1], CONTEXT_SIZE, stoi, TERM_TOK)\n",
    "dev_dataset = TrigramDataset(words[n1:n2], CONTEXT_SIZE, stoi, TERM_TOK)\n",
    "test_dataset = TrigramDataset(words[n2:], CONTEXT_SIZE, stoi, TERM_TOK)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [B, C]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"train dataset size: {len(train_dataset)}\")\n",
    "print(f\"dev dataset size: {len(dev_dataset)}\")\n",
    "print(f\"test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d96cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mosly a copy of torch Linear impl\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # stored as transpose because forward pass is x @ W.T\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # pytorch's weird default init, prolly from torch7 days\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        nn.init.uniform_(self.weight, a=-bound, b=bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, a=-bound, b=bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"in_features={}, out_features={}, bias={}\".format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "\n",
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-05,\n",
    "        momentum: float | None = 0.1,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # for simplicity\n",
    "        self.affine = True\n",
    "        self.track_running_stats = True\n",
    "        # parameters\n",
    "        self.weight = nn.Parameter(torch.empty(num_features, **factory_kwargs))\n",
    "        self.bias = nn.Parameter(torch.empty(num_features, **factory_kwargs))\n",
    "        # buffers\n",
    "        self.running_mean: torch.Tensor | None\n",
    "        self.running_var: torch.Tensor | None\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, **factory_kwargs))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features, **factory_kwargs))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self) -> None:\n",
    "        self.running_mean.zero_()\n",
    "        self.running_var.fill_(1)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        self.reset_running_stats()\n",
    "        torch.nn.init.ones_(self.weight)\n",
    "        torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"{num_features}, eps={eps}, momentum={momentum}, affine={affine}, track_running_stats={track_running_stats}\".format(\n",
    "            **self.__dict__\n",
    "        )\n",
    "\n",
    "    def _check_input_dim(self, input: torch.Tensor) -> None:\n",
    "        if input.dim() != 2 and input.dim() != 3:\n",
    "            raise ValueError(f\"expected 2D or 3D input (got {input.dim()}D input)\")\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        self._check_input_dim(input)\n",
    "        # training mode, use batch stats and accumulate\n",
    "        # eval mode, use running stats frozen when training finshed\n",
    "        if self.training:\n",
    "            xstd, xmean = torch.std_mean(input, dim=0, keepdim=True, unbiased=False)\n",
    "            xvar = xstd**2\n",
    "            # no need torch no_grad because buffers are not parameters\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        input_hat = (input - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
    "        return self.weight * input_hat + self.bias\n",
    "\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.tanh(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2c9cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized and saved to  model_initial.pth\n",
      "NgramLM(\n",
      "  (embedding): Embedding(27, 10)\n",
      "  (layers): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=30, out_features=100, bias=False)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "    (5): Linear(in_features=100, out_features=27, bias=False)\n",
      "  )\n",
      ")\n",
      "46970\n"
     ]
    }
   ],
   "source": [
    "def fc_layer(in_size: int, out_size: int, norm_layer: nn.Identity | BatchNorm1d, bias: bool = True) -> nn.Sequential:\n",
    "    \"\"\"Returns a stack of linear->norm->tanh layers.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        Linear(in_size, out_size, bias=bias),\n",
    "        norm_layer(out_size),\n",
    "        Tanh(),\n",
    "    )\n",
    "\n",
    "\n",
    "class NgramLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_size: int,\n",
    "        *,\n",
    "        batchnorm: bool = False,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        num_layers: int = NUM_LAYERS,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if batchnorm is False:\n",
    "            bias = True\n",
    "            norm_layer = nn.Identity\n",
    "        else:\n",
    "            bias = False\n",
    "            norm_layer = BatchNorm1d\n",
    "\n",
    "        layers = []\n",
    "        layers.append(fc_layer(context_size * embedding_dim, hidden_size, norm_layer=norm_layer, bias=bias))\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(fc_layer(hidden_size, hidden_size, norm_layer=norm_layer, bias=bias))\n",
    "        layers.append(Linear(hidden_size, vocab_size, bias=bias))\n",
    "        with torch.no_grad():\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    # make last layer less confident and all other scaled for tanh\n",
    "                    if layer is not layers[-1]:\n",
    "                        layer.weight.mul_(nn.init.calculate_gain(\"tanh\"))\n",
    "                    else:\n",
    "                        layer.weight.mul_(0.1)\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)  # (batch, context, embedding)\n",
    "        x = x.view(x.shape[0], -1)  # (batch, context*embedding)\n",
    "        logits = self.layers(x)  # (batch, vocab)\n",
    "        return logits\n",
    "\n",
    "\n",
    "if USE_BATCHNORM:\n",
    "    model = NgramLM(CONTEXT_SIZE, EMBEDDING_SIZE, vocab_size, batchnorm=True).to(device)\n",
    "else:\n",
    "    model = NgramLM(CONTEXT_SIZE, EMBEDDING_SIZE, vocab_size).to(device)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_INITIAL_PATH)\n",
    "print(\"Model initialized and saved to \", MODEL_INITIAL_PATH)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae3140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 3.295619 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    dataloader: data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # set train mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(\n",
    "    dataloader: data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    ") -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            test_loss += loss_fn(logits, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6678d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.293431  [   64/182625]\n",
      "loss: 2.834607  [ 6464/182625]\n",
      "loss: 2.974813  [12864/182625]\n",
      "loss: 2.605795  [19264/182625]\n",
      "loss: 2.428613  [25664/182625]\n",
      "loss: 2.409704  [32064/182625]\n",
      "loss: 2.384225  [38464/182625]\n",
      "loss: 2.511750  [44864/182625]\n",
      "loss: 2.410230  [51264/182625]\n",
      "loss: 2.538194  [57664/182625]\n",
      "loss: 2.328156  [64064/182625]\n",
      "loss: 2.357265  [70464/182625]\n",
      "loss: 2.388290  [76864/182625]\n",
      "loss: 1.997476  [83264/182625]\n",
      "loss: 2.219913  [89664/182625]\n",
      "loss: 2.332808  [96064/182625]\n",
      "loss: 2.278374  [102464/182625]\n",
      "loss: 2.457909  [108864/182625]\n",
      "loss: 2.372762  [115264/182625]\n",
      "loss: 2.467430  [121664/182625]\n",
      "loss: 2.345519  [128064/182625]\n",
      "loss: 2.230841  [134464/182625]\n",
      "loss: 2.200317  [140864/182625]\n",
      "loss: 2.389362  [147264/182625]\n",
      "loss: 2.031267  [153664/182625]\n",
      "loss: 2.285945  [160064/182625]\n",
      "loss: 2.365198  [166464/182625]\n",
      "loss: 2.168643  [172864/182625]\n",
      "loss: 2.227643  [179264/182625]\n",
      "Avg loss: 2.267794 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.373183  [   64/182625]\n",
      "loss: 2.191533  [ 6464/182625]\n",
      "loss: 2.538939  [12864/182625]\n",
      "loss: 1.885996  [19264/182625]\n",
      "loss: 2.347378  [25664/182625]\n",
      "loss: 2.066143  [32064/182625]\n",
      "loss: 2.075148  [38464/182625]\n",
      "loss: 2.058138  [44864/182625]\n",
      "loss: 2.018955  [51264/182625]\n",
      "loss: 2.194561  [57664/182625]\n",
      "loss: 2.516946  [64064/182625]\n",
      "loss: 2.347726  [70464/182625]\n",
      "loss: 2.192348  [76864/182625]\n",
      "loss: 2.431168  [83264/182625]\n",
      "loss: 2.073714  [89664/182625]\n",
      "loss: 2.179131  [96064/182625]\n",
      "loss: 2.078818  [102464/182625]\n",
      "loss: 2.218126  [108864/182625]\n",
      "loss: 2.353935  [115264/182625]\n",
      "loss: 2.178367  [121664/182625]\n",
      "loss: 2.131929  [128064/182625]\n",
      "loss: 2.100052  [134464/182625]\n",
      "loss: 2.082102  [140864/182625]\n",
      "loss: 2.280485  [147264/182625]\n",
      "loss: 2.081798  [153664/182625]\n",
      "loss: 2.305077  [160064/182625]\n",
      "loss: 2.127357  [166464/182625]\n",
      "loss: 2.212320  [172864/182625]\n",
      "loss: 2.591704  [179264/182625]\n",
      "Avg loss: 2.215787 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.100459  [   64/182625]\n",
      "loss: 2.219141  [ 6464/182625]\n",
      "loss: 2.412827  [12864/182625]\n",
      "loss: 2.007485  [19264/182625]\n",
      "loss: 2.332322  [25664/182625]\n",
      "loss: 2.131896  [32064/182625]\n",
      "loss: 2.144791  [38464/182625]\n",
      "loss: 2.020971  [44864/182625]\n",
      "loss: 2.279994  [51264/182625]\n",
      "loss: 2.316828  [57664/182625]\n",
      "loss: 2.401680  [64064/182625]\n",
      "loss: 2.065521  [70464/182625]\n",
      "loss: 2.078716  [76864/182625]\n",
      "loss: 2.137414  [83264/182625]\n",
      "loss: 2.274403  [89664/182625]\n",
      "loss: 2.395423  [96064/182625]\n",
      "loss: 2.496022  [102464/182625]\n",
      "loss: 2.209519  [108864/182625]\n",
      "loss: 2.406427  [115264/182625]\n",
      "loss: 2.164986  [121664/182625]\n",
      "loss: 2.175591  [128064/182625]\n",
      "loss: 2.046946  [134464/182625]\n",
      "loss: 2.366403  [140864/182625]\n",
      "loss: 2.327684  [147264/182625]\n",
      "loss: 2.195407  [153664/182625]\n",
      "loss: 2.173105  [160064/182625]\n",
      "loss: 2.024977  [166464/182625]\n",
      "loss: 2.201840  [172864/182625]\n",
      "loss: 2.384576  [179264/182625]\n",
      "Avg loss: 2.190141 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.150700  [   64/182625]\n",
      "loss: 2.004749  [ 6464/182625]\n",
      "loss: 2.395447  [12864/182625]\n",
      "loss: 2.202851  [19264/182625]\n",
      "loss: 2.178417  [25664/182625]\n",
      "loss: 2.019144  [32064/182625]\n",
      "loss: 2.230152  [38464/182625]\n",
      "loss: 2.311564  [44864/182625]\n",
      "loss: 2.001705  [51264/182625]\n",
      "loss: 2.188979  [57664/182625]\n",
      "loss: 2.311861  [64064/182625]\n",
      "loss: 2.249541  [70464/182625]\n",
      "loss: 2.259141  [76864/182625]\n",
      "loss: 2.329173  [83264/182625]\n",
      "loss: 2.297015  [89664/182625]\n",
      "loss: 2.257657  [96064/182625]\n",
      "loss: 1.852954  [102464/182625]\n",
      "loss: 2.237968  [108864/182625]\n",
      "loss: 2.060870  [115264/182625]\n",
      "loss: 2.087399  [121664/182625]\n",
      "loss: 2.147310  [128064/182625]\n",
      "loss: 2.231876  [134464/182625]\n",
      "loss: 2.356928  [140864/182625]\n",
      "loss: 2.203404  [147264/182625]\n",
      "loss: 2.343604  [153664/182625]\n",
      "loss: 2.089812  [160064/182625]\n",
      "loss: 2.376498  [166464/182625]\n",
      "loss: 2.113614  [172864/182625]\n",
      "loss: 1.950528  [179264/182625]\n",
      "Avg loss: 2.178501 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.434292  [   64/182625]\n",
      "loss: 2.256215  [ 6464/182625]\n",
      "loss: 2.069425  [12864/182625]\n",
      "loss: 2.587073  [19264/182625]\n",
      "loss: 2.007762  [25664/182625]\n",
      "loss: 2.102382  [32064/182625]\n",
      "loss: 2.349951  [38464/182625]\n",
      "loss: 2.049597  [44864/182625]\n",
      "loss: 2.233508  [51264/182625]\n",
      "loss: 2.166276  [57664/182625]\n",
      "loss: 2.065328  [64064/182625]\n",
      "loss: 1.990175  [70464/182625]\n",
      "loss: 2.043393  [76864/182625]\n",
      "loss: 2.123913  [83264/182625]\n",
      "loss: 2.111882  [89664/182625]\n",
      "loss: 2.089417  [96064/182625]\n",
      "loss: 2.259715  [102464/182625]\n",
      "loss: 2.175465  [108864/182625]\n",
      "loss: 2.256061  [115264/182625]\n",
      "loss: 2.567518  [121664/182625]\n",
      "loss: 1.963758  [128064/182625]\n",
      "loss: 1.964784  [134464/182625]\n",
      "loss: 2.172876  [140864/182625]\n",
      "loss: 2.075631  [147264/182625]\n",
      "loss: 2.135839  [153664/182625]\n",
      "loss: 2.392288  [160064/182625]\n",
      "loss: 2.126986  [166464/182625]\n",
      "loss: 2.210043  [172864/182625]\n",
      "loss: 2.138630  [179264/182625]\n",
      "Avg loss: 2.160488 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c335e412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fakeels.\n",
      "lia.\n",
      "zena.\n",
      "niky.\n",
      "lilo.\n",
      "mivrehus.\n",
      "briel.\n",
      "cpiy.\n",
      "aryanna.\n",
      "javurie.\n",
      "plie.\n",
      "srion.\n",
      "kendevaciuwarto.\n",
      "jalea.\n",
      "bryz.\n",
      "nauln.\n",
      "marelin.\n",
      "ston.\n",
      "bree.\n",
      "yoz.\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        context = [0] * CONTEXT_SIZE\n",
    "        out = \"\"\n",
    "        while True:\n",
    "            logits = model(torch.tensor([context]))\n",
    "            probs = logits.softmax(dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            context = context[1:] + [ix]\n",
    "            out += itos[ix]\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59aeb21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 2.159508 \n",
      "\n",
      "Avg loss: 2.160488 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(dev_dataloader, model, loss_fn)\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad03b3f",
   "metadata": {},
   "source": [
    "No BatchNorm:\n",
    "- Avg loss: 2.212028 \n",
    "- Avg loss: 2.207326 \n",
    "\n",
    "With BatchNorm:\n",
    "\n",
    "- Avg loss: 2.159508 \n",
    "- Avg loss: 2.160488 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097646b6",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing activations and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eced84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 3.295619 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_BATCHNORM:\n",
    "    model = NgramLM(CONTEXT_SIZE, EMBEDDING_SIZE, vocab_size, batchnorm=True).to(device)\n",
    "else:\n",
    "    model = NgramLM(CONTEXT_SIZE, EMBEDDING_SIZE, vocab_size).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_INITIAL_PATH, weights_only=True))\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611d1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlib.Path(MODEL_INITIAL_PATH).unlink(missing_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kamui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
